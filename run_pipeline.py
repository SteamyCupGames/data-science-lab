# run_pipeline.py

from Source.scraper import scrape_unimart
from Source.mongodb_connection import save_to_mongo
from Source.clean_dataset import clean_dataset
from Source.plot_creator import analyze_data

def main():
    print("ğŸš€ Starting Data Science Pipeline...\n")

    print("ğŸ” 1. Extracting data from Unimart...")
    products = scrape_unimart()
    print(f"âœ… {len(products)} products extracted.")

    print("\nğŸ§¹ 2. Cleaning dataset...")
    df_clean = clean_dataset(products)
    print(f"âœ… Dataset clean with {df_clean.shape[0]} rows.")

    print("\nğŸ’¾ 3. Saving cleaned data to MongoDB...")
    save_to_mongo(df_clean.to_dict(orient='records'))
    print("âœ… Clean data saved to MongoDB.")

    print("\nğŸ“Š 4. Analyzing and visualizing data...")
    analyze_data(df_clean)
    print("âœ… Analysis completed. Plots generated.")

    print("\nğŸ‰ Pipeline finished successfully.")

if __name__ == "__main__":
    main()
